{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import unicodedata as ud\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "import csv\n",
    "from itertools import izip\n",
    "import HTMLParser\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanTweet(originalTweet):\n",
    "    htmlParser = HTMLParser.HTMLParser()\n",
    "\n",
    "    tweet = originalTweet\n",
    "    #tweet = htmlParser.unescape(originalTweet);\n",
    "    #tweet = tweet.decode('windows-1252').encode('ascii', 'ignore')\n",
    "    #tweet = tweet.decode('windows-1252')\n",
    "    #tweet = tweet.decode(\"utf8\").encode('ascii', 'ignore')\n",
    "    #tweet = re.sub(r'[^\\x00-\\xFF]+', r'', tweet)\n",
    "    #tweet = re.sub(r'[^\\x00-\\x7F]+', r'', tweet)\n",
    "    #tweet = tweet.decode('utf-8').strip()\n",
    "    #tweet = tweet.decode('unicode_escape').encode('ascii','ignore')\n",
    "    #tweet = tweet.encode('ascii','ignore')\n",
    "    tweet = ''.join([i if ord(i) < 128 else ' ' for i in tweet])\n",
    "    \n",
    "    # remove URLs in tweet\n",
    "    tweet = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet)\n",
    "\n",
    "    # remove HTML tags from tweet\n",
    "    tweet = re.sub('<[^<]+?>', '', tweet)\n",
    "\n",
    "    # remove strings starting with @ in tweet\n",
    "    tweet = re.sub(r'(\\s)@\\w+', r'', tweet)\n",
    "\n",
    "    # remove exclamations\n",
    "    tweet = re.sub(r'!', r' ', tweet)\n",
    "\n",
    "    # separates words joined with capital words.\n",
    "    # E.g. DisplayIsAweson to Display Is Awesom\n",
    "    #tweet = \" \".join(re.findall('[A-Z][^A-Z]*', tweet));\n",
    "\n",
    "    # remove extra white spaces\n",
    "    tweet = re.sub(r'\\s+', r' ', tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9b374d122a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\stuwrkr-kdurge2\\\\Downloads\\\\obamaTweets.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\shekh\\\\Google Drive\\\\UIC\\\\UIC Fall 2016 Semester\\\\Data Mining and Text  Mining\\\\twitteranalysis\\\\data\\\\trainObama_3_columns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5624"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawTweetsSeries = data['Anootated tweet'];\n",
    "tweetLabels = data['Class'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawTweetsList = rawTweetsSeries.tolist()\n",
    "tweetLabelList = tweetLabels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5624"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawTweetsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean all the tweets\n",
    "i = 0;\n",
    "cleanedTweetsList = []\n",
    "for tweet in rawTweetsList:\n",
    "    #tweet.encode('utf-8').strip()\n",
    "    #tweet = tweet.decode(\"utf8\").encode('ascii', 'ignore')\n",
    "    #print (i ,),\n",
    "    cleanedTweet = cleanTweet(tweet).encode('ascii', 'ignore').strip();\n",
    "    cleanedTweetsList.append(cleanedTweet);\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@realDonaldTrump I agree we cant afford 4 more years of Obama'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedTweetsList[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5624\n",
      "5624\n"
     ]
    }
   ],
   "source": [
    "print (i)\n",
    "print (len(cleanedTweetsList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('cleanedRomney.csv', 'wb') as f:\n",
    "#    writer = csv.writer(f)\n",
    "#    writer.writerows(izip(cleanedTweetsList, tweetLabelList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainTweets, testTweets, trainTweetLabels, testTweetLabels = train_test_split(cleanedTweetsList, tweetLabelList, \n",
    "                                                                              test_size=0.25, random_state = 40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4218"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainTweetLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create feature vectors\n",
    "vectorizer = TfidfVectorizer(min_df=5,\n",
    "                             max_df = 0.75,\n",
    "                             sublinear_tf=True,\n",
    "                             use_idf=True,\n",
    "                             stop_words=u'english',\n",
    "                             analyzer='word',ngram_range=(1,5),lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vectors = vectorizer.fit_transform(trainTweets);\n",
    "test_vectors = vectorizer.transform(testTweets);\n",
    "total_vectors = vectorizer.fit_transform(cleanedTweetsList);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zero', u'indonesia', u'inept civil servant', u'seizure americans bank accounts', u'seizure americans bank', u'seizure americans', u'seizure', u'info', u'inquirer', u'interesting obama', u'interviews', u'investments', u'jesus', u'jewish', u'jews', u'job president', u'jobs overseas', u'johnson', u'jumped', u'seals']\n"
     ]
    }
   ],
   "source": [
    "type(train_vectors)\n",
    "features = vectorizer.get_feature_names();\n",
    "indices = np.argsort(vectorizer.idf_)[::-1]\n",
    "top_n = 20\n",
    "top_features = [features[i] for i in indices[:top_n]]\n",
    "print top_features\n",
    "#print(train_vectors);\n",
    "\n",
    "#len(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classifier_rbf = svm.SVC()      default kernel RBF. Very poor results, classify all tweets as -1\n",
    "#classifier_rbf = svm.SVC(kernel='linear')     decent results, ~50-60 accuracy\n",
    "#classifier_rbf = svm.LinearSVC()\n",
    "#knn = KNeighborsClassifier()\n",
    "#logistic = linear_model.LogisticRegression(C=1e5)\n",
    "classifier_sgd = linear_model.SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_sgd.fit(train_vectors, trainTweetLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_rbf = classifier_sgd.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictedLabelList = prediction_rbf.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classActual = np.array(testTweetLabels)\n",
    "\n",
    "classPredicted = np.array(predictedLabelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1406"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classActual\n",
    "len(classActual)\n",
    "\n",
    "classPredicted\n",
    "len(classPredicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_names = ['-1', '0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.60125261,  0.54545455,  0.59911894]),\n",
       " array([ 0.60631579,  0.52332657,  0.62100457]),\n",
       " array([ 0.60377358,  0.53416149,  0.60986547]),\n",
       " array([475, 493, 438]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prfs(classActual, classPredicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1406\n",
      "1406\n"
     ]
    }
   ],
   "source": [
    "print (len(predictedLabelList))\n",
    "print (len(testTweetLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = 0;\n",
    "for i in range(len(predictedLabelList)):\n",
    "    if(predictedLabelList[i] == testTweetLabels[i]):\n",
    "        correct += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818\n"
     ]
    }
   ],
   "source": [
    "print (correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1L"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedLabelList[470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1L: 479, 0L: 473, 1L: 454})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "counter=collections.Counter(predictedLabelList)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 493, -1: 475, 1: 438})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "counter=collections.Counter(testTweetLabels)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58179231863442393"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_sgd.score(test_vectors, testTweetLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.60      0.61      0.60       475\n",
      "          0       0.55      0.52      0.53       493\n",
      "          1       0.60      0.62      0.61       438\n",
      "\n",
      "avg / total       0.58      0.58      0.58      1406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(testTweetLabels, predictedLabelList, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvScores = cross_val_score(classifier_sgd, total_vectors, tweetLabelList, cv=10,scoring='f1_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validations scores:  [ 0.55622357  0.48660356  0.57033504  0.57365497  0.55203354  0.59052206\n",
      "  0.6042658   0.50904247  0.5842544   0.56609024]\n"
     ]
    }
   ],
   "source": [
    "print (\"Cross validations scores: \"),\n",
    "print (cvScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean: ', 0.55930256570510983)\n",
      "('Minimum: ', 0.48660355600289507)\n",
      "('Maximum: ', 0.60426580474077662)\n"
     ]
    }
   ],
   "source": [
    "print (\"Mean: \", cvScores.mean())\n",
    "print (\"Minimum: \", cvScores.min())\n",
    "print (\"Maximum: \", cvScores.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
